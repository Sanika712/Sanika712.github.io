# -*- coding: utf-8 -*-
"""RA-Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MVpDBPEGkTH7lfD5j72lwrQbzbgkW4q-
"""

# Install required libraries
!pip install transformers datasets accelerate deepspeed peft bitsandbytes --upgrade

# Import logging module
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Check bitsandbytes compatibility
import bitsandbytes
print(f"BitsAndBytes version: {bitsandbytes.__version__}")

# Import libraries
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from datasets import Dataset
from sklearn.metrics import accuracy_score
from torch import nn
from transformers import Trainer, TrainingArguments
import logging
import os

# Define model name and tokenizer
model_name = "unsloth/llama-3-8b-bnb-4bit"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Configure quantization
bnb_config = BitsAndBytesConfig(load_in_8bit=True)

# Clear GPU memory before loading the model
torch.cuda.empty_cache()

import torch
print(torch.cuda.is_available())

# Load the model with quantization
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        low_cpu_mem_usage=True
    )
    model.gradient_checkpointing_enable()  # Enable gradient checkpointing
    model.config.use_cache = False  # Disable cache for compatibility
except RuntimeError as e:
    raise RuntimeError(f"Error loading model: {e}")

# Define the QLoRAAdapter class for LoRA fine-tuning
class QLoRAAdapter(nn.Module):
    def __init__(self, original_model, rank=8):
        super().__init__()
        self.transformer = original_model
        self.lora_A = nn.Linear(original_model.config.hidden_size, rank, bias=False)
        self.lora_B = nn.Linear(rank, original_model.config.hidden_size, bias=False)
        nn.init.kaiming_uniform_(self.lora_A.weight, a=5**0.5)
        nn.init.zeros_(self.lora_B.weight)
        # Ensure LoRA layers are in the same dtype as the model
        self.lora_A = self.lora_A.to(original_model.dtype)
        self.lora_B = self.lora_B.to(original_model.dtype)

    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, labels=None, **kwargs):
        if inputs_embeds is None:
            inputs_embeds = self.transformer.get_input_embeddings()(input_ids)
        # Ensure inputs_embeds are in the same dtype as the model
        inputs_embeds = inputs_embeds.to(self.transformer.dtype)

        lora_delta = self.lora_B(self.lora_A(inputs_embeds))
        updated_input_embeddings = inputs_embeds + lora_delta
        outputs = self.transformer(
            input_ids=None,
            attention_mask=attention_mask,
            inputs_embeds=updated_input_embeddings,
            labels=labels,
            **kwargs
        )
        return (outputs.loss, outputs) if labels is not None else outputs

# Set device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the QLoRAAdapter with the loaded model
qlora_adapter = QLoRAAdapter(model).to(device)

# Prepare dataset

data = {
    "text": [
        "Survey the area and capture images every 10 meters.",
        "Integrate LiDAR sensor with drone for terrain mapping.",
        "Fly to coordinates (45.0, -93.0), then capture video.",
        "Return to base after completing the mission.",
        "Monitor altitude and adjust for wind speed during flight.",
        "Enable obstacle detection sensor for safe navigation."
    ]
}
dataset = Dataset.from_dict(data).train_test_split(test_size=0.2)

# Tokenize both text (input) and labels (output)
def tokenize_function(examples):
    inputs = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

    inputs["labels"] = outputs["input_ids"]
    return inputs

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=1000,
    logging_dir="./logs",
    logging_steps=50,
    save_total_limit=3,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=2,
    learning_rate=5e-5,
    report_to="tensorboard",
    remove_unused_columns=False,
    optim="adamw_hf",
)

# Compute metrics
def compute_metrics(p):
    preds = torch.argmax(p.predictions, axis=1)
    return {"accuracy": accuracy_score(p.label_ids, preds)}

# Initialize Trainer
trainer = Trainer(
    model=qlora_adapter,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

# Save the model and tokenizer
qlora_adapter.transformer.save_pretrained("./fine_tuned_model")  # Save original model
tokenizer.save_pretrained("./fine_tuned_model")

print("Fine-tuning complete and model saved.")

# Import necessary modules
import torch

# Enable CUDA device-side assertions for debugging
import os
os.environ['TORCH_USE_CUDA_DSA'] = '1'

# Check if CUDA is available and print memory usage
if torch.cuda.is_available():
    print("CUDA available. Using GPU.")
    device = torch.device("cuda")
    print(f"Memory allocated before: {torch.cuda.memory_allocated()} bytes")
else:
    print("CUDA not available. Using CPU.")
    device = torch.device("cpu")

# Clear GPU memory and check the allocated memory before and after
try:
    print(f"Memory allocated before clearing cache: {torch.cuda.memory_allocated()} bytes")
    torch.cuda.empty_cache()  # Clear GPU memory cache
    print(f"Memory allocated after clearing cache: {torch.cuda.memory_allocated()} bytes")
except RuntimeError as e:
    print(f"Error during memory cleanup: {e}")

# Function to generate Minispec commands using the fine-tuned model
def generate_minispec_commands(input_text):
    try:
        # Tokenize the input and ensure tensor has no gradient tracking
        inputs = tokenizer(input_text, return_tensors="pt").to(device)
        inputs["input_ids"].requires_grad_(False)

        # Ensure `use_cache=False` to avoid conflicts
        model.config.use_cache = False

        # Check input tensor shape before passing to the model
        print(f"Input tensor shape: {inputs['input_ids'].shape}")

        with torch.no_grad():  # Inference does not require gradient computation
            # Perform the generation
            outputs = model.generate(
                input_ids=inputs["input_ids"],
                max_length=50,
                num_beams=5,  # Beam search for better output
                early_stopping=True
            )

            # Decode the generated output
            decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Return the formatted Minispec command
            return convert_to_minispec(decoded_output)

    except RuntimeError as e:
        print(f"Error during generation: {e}")
        return None

# Helper function to format the model's output as a Minispec command
def convert_to_minispec(generated_text):
    generated_text = generated_text.lower()

    # Example rules to convert to Minispec format:
    if "survey" in generated_text and "area" in generated_text:
        generated_text = "tc(180); tc(180);"  # Add specific command for surveying

    if "return" in generated_text and "base" in generated_text:
        generated_text += " g('airplane')"  # Add return to base command

    # Return the formatted Minispec command
    return f"Generated Minispec Command: {generated_text}"

# Example Usage:
prompt = "Generate code to survey an area and return to the base."
generated_command = generate_minispec_commands(prompt)
print(f"Input: {prompt}")
print(f"Output: {generated_command}")

print(f"Input tensor shape: {inputs['input_ids'].shape}")