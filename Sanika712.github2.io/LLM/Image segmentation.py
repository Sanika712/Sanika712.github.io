# -*- coding: utf-8 -*-
"""RA-Task 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcyolU_7rfAxnyP7e6pvFWY_uDyQXKWO
"""

!rm -rf LLMSeg
!rm -rf LISA
# Install necessary libraries
!pip install --upgrade transformers
!pip install torch torchvision Pillow
from IPython.display import display

# Clone the LLM-Seg repository
!rm -rf LLMSeg
!git clone https://github.com/wangjunchi/LLMSeg.git

# Commented out IPython magic to ensure Python compatibility.
# Change directory to LLMSeg
# %cd LLMSeg

# Add LLMSeg directory to Python's path
import sys
sys.path.append("/content/LLMSeg")

# Mount Google Drive to access SAM model checkpoint
from google.colab import drive
drive.mount('/content/drive')

# Import required libraries
import torch
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files

# Check for device compatibility
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Import and initialize SAM model
from model.segment_anything import sam_model_registry, SamPredictor
sam_checkpoint_path = "/content/drive/MyDrive/sam_model/sam_model.pth"

try:
    sam_model = sam_model_registry["vit_h"](checkpoint=sam_checkpoint_path)
    predictor = SamPredictor(sam_model)
    predictor.model.to(device)
    print("SAM model loaded successfully.")
except Exception as e:
    print(f"Error loading SAM model: {e}")

# Upload image
uploaded = files.upload()

# Select the uploaded image path
for filename in uploaded.keys():
    image_path = filename
    print(f"Uploaded image: {image_path}")

# Select the uploaded image path
for filename in uploaded.keys():
    image_path = filename
    print(f"Uploaded image: {image_path}")

# Import BLIP-2 for reasoning
from transformers import Blip2Processor, Blip2ForConditionalGeneration

try:
    processor = Blip2Processor.from_pretrained("Salesforce/blip2-flan-t5-xl")
    blip2_model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-flan-t5-xl")
    blip2_model.to(device)
    print("BLIP-2 model loaded successfully.")
except Exception as e:
    print(f"Error loading BLIP-2 model: {e}")

def generate_reasoning_segmentation(image_path, prompt, sam_predictor, blip_model, processor, device):
    try:
        # Step 1: Load the image
        image = Image.open(image_path).convert("RGB")
        image_np = np.array(image)
        sam_predictor.set_image(image_np)

        # Step 2: Process the image and text using the processor
        inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)

        # Step 3: Generate reasoning using BLIP-2
        outputs = blip2_model.generate(
            **inputs,
            max_length=256
        )
        reasoning = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Step 4: Generate segmentation masks using SAM
        input_point = np.array([[100, 100]])  # Modify to dynamic point selection
        input_label = np.array([1])
        masks, _, _ = sam_predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
        )

        # Step 5: Visualize results
        mask_resized = Image.fromarray(masks[0]).resize(image.size, Image.NEAREST)
        plt.figure(figsize=(10, 5))
        plt.imshow(image)
        plt.imshow(mask_resized, alpha=0.5, cmap="jet")
        plt.title(f"Reasoning: {reasoning}")
        plt.axis("off")
        plt.show()

        return reasoning, masks

    except Exception as e:
        print(f"Error during reasoning and segmentation: {e}")
        return None, None

# Example prompt for reasoning
prompt = "Identify and segment the dog in this image."

# Call the function to generate reasoning and segmentation
reasoning, masks = generate_reasoning_segmentation(image_path, prompt, predictor, blip2_model, processor, device)