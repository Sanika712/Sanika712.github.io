# -*- coding: utf-8 -*-
"""walmartpostal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VgYtIFnozkKvQQwNC4lAqISCOmGNCaci
"""

# Importing necessary libraries
from google.colab import files
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Uploading the file
uploaded = files.upload()

# Read the file
df = pd.read_csv('WalmartFinal.csv')
print(df.head())

# Check for missing values
print(df.isnull().sum())

#handling missing values of postal code
mode_postal_code = df['Postal Code'].mode()[0]  # Get the most frequent postal code
df['Postal Code'] = df['Postal Code'].fillna(mode_postal_code)

# Check for missing values
print(df.isnull().sum())

# Display column names
print(df.columns)

#checking the columns
print(df.dtypes)

#converting date to datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Drop rows with missing Date values
df = df.dropna(subset=['Date'])
# Extract time-related features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['Weekday'] = df['Date'].dt.weekday
df['Quarter'] = df['Date'].dt.quarter
df['Week_of_Year'] = df['Date'].dt.isocalendar().week
df['Day_Name'] = df['Date'].dt.day_name()

# Print the updated DataFrame
print(df[['Date', 'Year', 'Month', 'Day', 'Weekday', 'Quarter', 'Week_of_Year', 'Day_Name']].head())

# Apply label encoding to 'Day_Name' column
le = LabelEncoder()
df['Day_Name'] = le.fit_transform(df['Day_Name'])

# Encode the 'Store' column as it's categorical
df['Store'] = df['Store'].astype('category')
df['Store'] = df['Store'].cat.codes  # Label encoding

# Select all features except the target variable
X = df.drop(columns=['Weekly_Sales', 'Date'])

# Set the target variable (y) as 'Weekly_Sales'
y = df['Weekly_Sales']

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the hyperparameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'alpha': [0, 1, 10]  # L2 regularization term
}

# Initialize the model
xg_reg = XGBRegressor(objective='reg:squarederror')

# Initialize GridSearchCV with cross-validation
grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Get the best parameters from GridSearchCV
best_params = grid_search.best_params_
print(f"Best parameters found: {best_params}")

# Get the best model from the GridSearchCV
best_model = grid_search.best_estimator_

# 4. Train the best model
best_model.fit(X_train, y_train)

# 5. Make predictions on the test set using the best model
y_pred_best = best_model.predict(X_test)

# Evaluate the best model's performance
mse_best = mean_squared_error(y_test, y_pred_best)
rmse_best = np.sqrt(mse_best)
mae_best = mean_absolute_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)

# Print evaluation metrics for the best model
print(f'Best Model - Root Mean Squared Error (RMSE): {rmse_best}')
print(f'Best Model - Mean Absolute Error (MAE): {mae_best}')
print(f'Best Model - R-squared (RÂ²): {r2_best}')
print(f'Best Model - Mean Squared Error (MSE): {mse_best}')

# Create a DataFrame to store the actual and predicted values
results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})

# Display the first few rows of the DataFrame
print(results_df.head())

# Optionally, you can export the results to a CSV file for further analysis
results_df.to_csv('actual_vs_predicted.csv', index=False)

# Show the first few rows for inspection
results_df.head()

# Create a copy of the test data
X_test_copy = X_test.copy()

# List of temperature values to test
temperature_values = np.linspace(X_test['Temperature'].min(), X_test['Temperature'].max(), 10)

# Store the predictions for each temperature value
predictions_temp = []

# Loop through each temperature value, keeping other features constant
for temp in temperature_values:
    X_test_copy['Temperature'] = temp
    pred = best_model.predict(X_test_copy)
    predictions_temp.append(pred)

# Convert predictions to a DataFrame for easier visualization
temperature_results = pd.DataFrame({'Temperature': temperature_values, 'Predicted_Weekly_Sales': np.mean(predictions_temp, axis=1)})

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(temperature_results['Temperature'], temperature_results['Predicted_Weekly_Sales'], marker='o', color='blue')
plt.title('Predictions Based on Temperature', fontsize=16)
plt.xlabel('Temperature', fontsize=12)
plt.ylabel('Predicted Weekly Sales', fontsize=12)
plt.grid(True)
plt.show()

# Let's pick a different postal code from the test set
postal_code_to_predict = 22304

# Filter the test set for that specific postal code
X_test_postal = X_test[X_test['Postal Code'] == postal_code_to_predict]

# Check if the filtered test set is empty
if not X_test_postal.empty:
    # Make predictions for the selected postal code
    y_pred_postal = best_model.predict(X_test_postal)

    # Get the corresponding actual values for the selected postal code
    y_test_postal = y_test[X_test['Postal Code'] == postal_code_to_predict]

    # Create a DataFrame for actual vs predicted values
    results_postal_df = pd.DataFrame({'Actual': y_test_postal, 'Predicted': y_pred_postal})

    # Display the results
    print(results_postal_df)

    # Visualization: Actual vs Predicted Weekly Sales for the Postal Code
    plt.figure(figsize=(10, 6))

    # Plot actual and predicted values side by side
    results_postal_df.plot(kind='bar', figsize=(10, 6))

    plt.title(f'Actual vs Predicted Weekly Sales for Postal Code {postal_code_to_predict}', fontsize=16)
    plt.xlabel('Index', fontsize=12)
    plt.ylabel('Weekly Sales', fontsize=12)
    plt.xticks(rotation=45)
    plt.legend(['Actual', 'Predicted'])

    plt.tight_layout()
    plt.show()
else:
    print(f"No data found for postal code {postal_code_to_predict} in the test set.")

# 1. Plotting Actual vs Predicted Values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue')
plt.title('Actual vs Predicted Weekly Sales', fontsize=16)
plt.xlabel('Actual Weekly Sales', fontsize=12)
plt.ylabel('Predicted Weekly Sales', fontsize=12)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)  # Diagonal line
plt.grid(True)
plt.show()

# 2. Error Dist

# Get the feature importance
booster = best_model.get_booster()

# Get feature importance scores
importance = booster.get_score(importance_type='weight')

# Convert the importance dictionary to a pandas DataFrame for easier plotting
importance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])

# Sort the importance by the score
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the top 10 features
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'][:10], importance_df['Importance'][:10], color='blue')
plt.xlabel('Importance', fontsize=12)
plt.title('Top 10 Feature Importance', fontsize=16)
plt.show()

# 1. Choose the year you want to predict for
year_to_predict = 2011

# Filter the original dataframe for the chosen year
df_year_to_predict = df[df['Year'] == year_to_predict]

# Prepare the feature data (X) for the year you want to predict, excluding 'Weekly_Sales' and 'Date'
X_predict = df_year_to_predict.drop(columns=['Weekly_Sales', 'Date'])

# 2. Make predictions using the trained model
predicted_sales = best_model.predict(X_predict)  # Use X_predict, not X_test_copy

# Add the predicted sales to the dataframe
df_year_to_predict['Predicted_Weekly_Sales'] = predicted_sales

# 3. Group by 'Month' and sum the actual and predicted weekly sales for each month
monthly_actual_sales = df_year_to_predict.groupby('Month')['Weekly_Sales'].sum()
monthly_predicted_sales = df_year_to_predict.groupby('Month')['Predicted_Weekly_Sales'].sum()

# 4. Combine actual and predicted sales into one DataFrame
monthly_sales_comparison = pd.DataFrame({
    'Actual Sales': monthly_actual_sales,
    'Predicted Sales': monthly_predicted_sales
})

# 5. Visualize the actual and predicted sales for the selected year
plt.figure(figsize=(12, 6))

# Plotting actual and predicted sales
monthly_sales_comparison.plot(kind='bar', color=['blue', 'orange'], ax=plt.gca())

# Customize the plot
plt.title(f'Actual vs Predicted Monthly Sales for {year_to_predict}', fontsize=16)
plt.xlabel('Month', fontsize=12)
plt.ylabel('Total Weekly Sales', fontsize=12)
plt.xticks(ticks=np.arange(12), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)
plt.grid(True)
plt.tight_layout()
plt.legend(['Actual Sales', 'Predicted Sales'], fontsize=12)
plt.show()

# Print the actual and predicted monthly sales for the selected year
print(f"Actual and Predicted Monthly Sales for {year_to_predict}:")
print(monthly_sales_comparison)

# Add month and year information to test data
X_test_with_date = X_test.copy()
X_test_with_date['Month'] = X.loc[X_test.index, 'Month']
X_test_with_date['Year'] = X.loc[X_test.index, 'Year']

# Combine predictions with the actual test data
results = pd.DataFrame({
    'Year': X_test_with_date['Year'],
    'Month': X_test_with_date['Month'],
    'Actual Sales': y_test,
    'Predicted Sales': y_pred
})

# Take year as input from the user
try:
    input_year = int(input("Enter the year for which you want to view sales data: "))
except ValueError:
    print("Invalid year input. Please enter a valid integer year.")
    exit()

# Filter data for the given year
yearly_results = results[results['Year'] == input_year]

# Check if there is data for the given year
if yearly_results.empty:
    print(f"No data available for the year {input_year}.")
else:
    # Group by month and calculate the sum of sales
    monthwise_results = yearly_results.groupby('Month')[['Actual Sales', 'Predicted Sales']].sum().reset_index()

    # Format sales columns to two decimal places and disable scientific notation
    monthwise_results['Actual Sales'] = monthwise_results['Actual Sales'].apply(lambda x: f"{x:,.2f}")
    monthwise_results['Predicted Sales'] = monthwise_results['Predicted Sales'].apply(lambda x: f"{x:,.2f}")

    # Print the formatted results
    print(monthwise_results)

    # Convert back to numeric for plotting
    monthwise_results['Actual Sales'] = monthwise_results['Actual Sales'].str.replace(',', '').astype(float)
    monthwise_results['Predicted Sales'] = monthwise_results['Predicted Sales'].str.replace(',', '').astype(float)

    # Visualization
    plt.figure(figsize=(12, 6))
    plt.plot(monthwise_results['Month'], monthwise_results['Actual Sales'], marker='o', label='Actual Sales', color='blue')
    plt.plot(monthwise_results['Month'], monthwise_results['Predicted Sales'], marker='o', label='Predicted Sales', color='orange')
    plt.xlabel('Month', fontsize=12)
    plt.ylabel('Total Sales', fontsize=12)
    plt.title(f'Month-wise Actual vs Predicted Sales for {input_year}', fontsize=16)
    plt.xticks(monthwise_results['Month'], fontsize=10)
    plt.legend(fontsize=12)
    plt.grid()
    plt.tight_layout()
    plt.show()